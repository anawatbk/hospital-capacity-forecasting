{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/anawatbk/hospital-capacity-forecasting/blob/main/final_dev.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõèÔ∏èüìà California COVID-19 Hospital Bed Capacity Forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Anawat Putwanphen\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4\">Preprocessing</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Fit-scikit-learn-model-5\">Feature Engineering</a></span></li><li><span><a \n",
    "href=\"#Fitting-and-Optimizing-Machine-Learning-Models-&-Hyperparameters\" data-toc-modified-id=\"Evaluation-Metric-6\">Fitting and Optimizing Machine Learning Models & Hyperparameters</a></span></li>\n",
    "<li><span><a href=\"#Models-Evaluation\" data-toc-modified-id=\"Evaluation-Metric-6\">Models Evaluation</a></span></li><li><span><a \n",
    "href=\"#Conclusion\" data-toc-modified-id=\"Evaluation-Metric-6\">Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Goal\n",
    "----\n",
    "Forecast California COVID-19 Hospital bed capacity 1-month ahead (weekly average).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COVID-19 Reported (weekly average) Patient Impact and Hospital Capacity by Facility by Department of Health and Human Services   \n",
    "https://healthdata.gov/dataset/covid-19-reported-patient-impact-and-hospital-capacity-facility\n",
    "<br>\n",
    "<br>\n",
    "**Dimension:** 10,578 observations x 93 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose            import *\n",
    "from sklearn.preprocessing      import *\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from sklearn.impute import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Lasso, SGDRegressor, BayesianRidge\n",
    "from sklearn.impute import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path = '../data/reported_hospital_capacity_admissions_facility_level_weekly_average_timeseries_20210228.csv'\n",
    "df = pd.read_csv(data_source_path, parse_dates = ['collection_week'])\\\n",
    "       .query('state == \"CA\"').sort_values(by=['hospital_pk', 'collection_week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "--------------\n",
    "Usually, Real world data is not in an ideal format for building machine learning models.  \n",
    "In this section, I cleaned up some data quality issues from source data.  \n",
    "For instance,\n",
    "1. Some hospitals have reported more covid cases than combined total cases which consis of non-covid + covid cases by the source's definition\n",
    "2. Select only hospitals who consistently have reported covid cases and bed capacity for the last 4 months (2020-11-06 until 2021-02-19)\n",
    "\n",
    "Building 4-step ahead forecasting models require us to transform our original data into appropriate format for time series modelling.  \n",
    "and clean up some data quality issues from source data.\n",
    "This steps including\n",
    "1. covert single target (time series) to 4-step-targets, for example, y | y+1 | y+2 | y+3\n",
    "2. filter out rows for last 3 rows for each hospital since it wasn't possible to get y+1 to y+3 of the latest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pipeline_preprocessing(df):\n",
    "    '''\n",
    "    All Preprocessing process which are not supported by Pipeline\n",
    "    1. drop rows of inconsistent covid cases report,\n",
    "       For example, covid cases > combined cases (which consist of non-covid + covid cases)\n",
    "    2. select rows of hospital who report covid cases in the last 4 months\n",
    "       (2020-11-06 until 2021-02-19) \n",
    "    '''\n",
    "    # 1 Drop inconsistent covid cases report hospital (inpatient < covid patient)\n",
    "    # column_sum/column_coverage(days) = weekly average\n",
    "    inconsistent_hospital_pk = df[(\n",
    "        (df['inpatient_beds_used_7_day_sum'] / df['inpatient_beds_used_7_day_coverage']) <      \n",
    "        (df['total_adult_patients_hospitalized_confirmed_and_suspected_covid_7_day_sum']\n",
    "            / df['total_adult_patients_hospitalized_confirmed_and_suspected_covid_7_day_coverage'])\n",
    "        )]['hospital_pk'].unique() \n",
    "    df = df[~np.isin(df['hospital_pk'], inconsistent_hospital_pk)] # drop inconsistent report hospital\n",
    "\n",
    "    # 2 keep only the hospitals who reported capacity every week for the last 4 month (2020-11-06 until 2021-02-19)\n",
    "    max_week_count = df.loc[df['collection_week'] > '2020-11-01']\\\n",
    "                       .groupby('hospital_pk').count().max()['collection_week']\n",
    "    hospital_pk_array = df.loc[df['collection_week'] > '2020-11-01']\\\n",
    "                          .groupby('hospital_pk').count().index.values\n",
    "    complete_hostital_mask = (df.loc[df['collection_week'] > '2020-11-01']\\\n",
    "                                .groupby('hospital_pk').count()['collection_week'] == max_week_count)\n",
    "    hospital_pk_array = hospital_pk_array[complete_hostital_mask]\n",
    "    df = df[np.isin(df['hospital_pk'], hospital_pk_array)].copy()\n",
    "    return df\n",
    "    \n",
    "def pre_pipeline_generate_multi_step_y(df):\n",
    "    '''\n",
    "    Generate  1 month-ahead (4 step-ahead) output target.\n",
    "    y | y+1 | y+2 | y+3\n",
    "    '''\n",
    "    # Transform target y into 4-step-ahead\n",
    "    y = df[['hospital_pk', 'inpatient_beds_used_7_day_sum', \n",
    "            'inpatient_beds_used_7_day_coverage']].copy()\n",
    "    # Weekly average bed used = bed used sum / bed used average\n",
    "    y['inpatient_bed_used'] = (y['inpatient_beds_used_7_day_sum'] / \n",
    "                               y['inpatient_beds_used_7_day_coverage'])\n",
    "    # Clean up target y\n",
    "    # By the source's definition -999999 values must be filled with 0\n",
    "    # By the source's definition Nan values must be filled with 0 \n",
    "    \n",
    "    y['inpatient_bed_used'] = y['inpatient_bed_used'].fillna(0)\n",
    "    y.loc[y['inpatient_bed_used'] < 0, 'inpatient_bed_used'] = 0  # fill -999999 values (or -114971 if average) with 0\n",
    "    \n",
    "    # Create leading columns --> yt+1, y+2, y+3\n",
    "    # The data must be groupby hospital_pk otherwise, we will get bed_used number from other hospital\n",
    "    step_ahead = 3\n",
    "    col = 'inpatient_bed_used'\n",
    "    for step in range(1, step_ahead+1):\n",
    "        y = y.assign(**{f'{col}+{step}': y.groupby('hospital_pk').shift(-step)[col]}) \n",
    "\n",
    "    # Drop the last 3 weeks of each hospital rows (X and y)\n",
    "    y = y.drop(['inpatient_beds_used_7_day_sum', \n",
    "                'inpatient_beds_used_7_day_coverage', 'hospital_pk'], \n",
    "               axis=1) # drop irrelavant columns from y\n",
    "    no_target_week_mask = y.isnull().sum(axis=1) > 0 # boolean array of the last 3 weeks\n",
    "    df = df[~no_target_week_mask].copy() \n",
    "    y = y.dropna()\n",
    "    y = y[['inpatient_bed_used', 'inpatient_bed_used+1', \n",
    "           'inpatient_bed_used+2', 'inpatient_bed_used+3']]\n",
    "    df['inpatient_bed_used'] = y['inpatient_bed_used'] # keep y in X to use for lagging features\n",
    "    return df.reset_index(drop=True), y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_pipeline_preprocessing(df)\n",
    "X_original, y_original = pre_pipeline_generate_multi_step_y(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "----------------\n",
    "Features for machine learning model are extracted in this step.\n",
    "The process including\n",
    "<br>\n",
    "**1. Create custom calculated columns**\n",
    "<br>\n",
    "Some information in Raw data can not be used directly for features. Some calculation must be performed.\n",
    "- Inpatient Bed Used (weekly average)\n",
    "- Total Covid inpatient (weekly average)\n",
    "- Inpatient Bed used (hospital average)\n",
    "\n",
    "**2. Select only columns relevant for this project**\n",
    "<br>\n",
    "<br>\n",
    "**3. Generate Lag features**\n",
    "<br>\n",
    "Example: x-4 | x-3 | x-2 | x-1\n",
    "<br>\n",
    "Create lag features for time series column including inpatient bed used, total covid inpatient.\n",
    "<br>\n",
    "<br>\n",
    "**4. Groupby Imputer**\n",
    "<br>\n",
    "Impute missing values of each column by its hospital's median ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom calculated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateCalculatedColumns(BaseEstimator, TransformerMixin):\n",
    "    ''' \n",
    "    Create new calculated columns pipelines.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\" \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        trans : pandas DataFrame\n",
    "            DataFrame contain new calculated columns.     \n",
    "        \"\"\"\n",
    "        self.before_shape = X.shape\n",
    "        \n",
    "        # Inpatient Bed Capacity (use to summarize capacity %)\n",
    "        X['inpatient_bed_capacity'] = X['inpatient_beds_7_day_sum'] / X['inpatient_beds_7_day_coverage']\n",
    "        # ICU Bed Capacity (use to summarize capacity %)\n",
    "        X['icu_bed_capacity'] = X['total_icu_beds_7_day_sum'] / X['total_icu_beds_7_day_coverage']\n",
    "        # Inpatient Bed Used\n",
    "        X['inpatient_bed_used'] = X['inpatient_beds_used_7_day_sum'] / X['inpatient_beds_used_7_day_coverage']\n",
    "        # Adult covid inpatient   \n",
    "        X['adult_inpatients_confirmed_n_suspected_covid'] = (\n",
    "            X['total_adult_patients_hospitalized_confirmed_and_suspected_covid_7_day_sum'] \n",
    "             / X['total_adult_patients_hospitalized_confirmed_and_suspected_covid_7_day_coverage'])\n",
    "        # Pediatric covid inpatient   \n",
    "        X['pediatric_inpatrients_confirmed_n_suspected_covid'] = (\n",
    "            X['total_pediatric_patients_hospitalized_confirmed_and_suspected_covid_7_day_sum']\n",
    "            / X['total_pediatric_patients_hospitalized_confirmed_and_suspected_covid_7_day_coverage'])\n",
    "        # Total covid inpatient (Adult+Pediatric)\n",
    "        X['total_inpatients_confirmed_n_suspected_covid'] = (\n",
    "            X['adult_inpatients_confirmed_n_suspected_covid'] \n",
    "            + X['pediatric_inpatrients_confirmed_n_suspected_covid'])\n",
    "        # From source's definition. Fill -999999 values by 0 values\n",
    "        X.loc[X['total_inpatients_confirmed_n_suspected_covid'] < 0, 'total_inpatients_confirmed_n_suspected_covid'] = 0\n",
    "        X.loc[X['inpatient_bed_used'] < 0, 'inpatient_bed_used'] = 0\n",
    "        # Average Inpatient Bed Used of each hospital\n",
    "        avg_inpatient_bed_used = X.groupby('hospital_pk').mean()[['inpatient_bed_used']]\\\n",
    "                                  .rename(columns={'inpatient_bed_used':'avg_inpatient_bed_used'})\n",
    "        \n",
    "        X = X.join(avg_inpatient_bed_used, on='hospital_pk')\n",
    "        trans = X.copy() \n",
    "        self.after_shape = trans.shape\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        \"\"\" Do nothing function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        y : default None\n",
    "                \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self  \n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only columns relevant for this project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. hospital primary key: 'hospital_pk'\n",
    "2. start of the week date: 'collection_week'\n",
    "3. Sub type of hospital: 'hospital_subtype'\n",
    "4. Type of hospital (metro/micro): 'is_metro_micro'\n",
    "5. Hospital's Inpatient Bed Capacity: 'inpatient_bed_capacity'\n",
    "6. Hospital's Inpatient Bed usage: 'inpatient_bed_used'\n",
    "7. Average Inpatient Bed usage: 'avg_inpatient_bed_used'\n",
    "8. Hospital's COVID Inpatient (confirmed & suspected): 'total_inpatients_confirmed_n_suspected_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify relavant columns and non-features column\n",
    "relevant_columns = [\n",
    "    'hospital_pk', 'collection_week', 'hospital_subtype', 'is_metro_micro',\n",
    "    'inpatient_bed_capacity', 'icu_bed_capacity', \n",
    "    'inpatient_bed_used', 'avg_inpatient_bed_used',\n",
    "    'total_inpatients_confirmed_n_suspected_covid',\n",
    "    ]\n",
    "\n",
    "# non features - hospital key, collection date, full capacity data (icu & Inpatient)\n",
    "non_feature_columns = ['hospital_pk', 'collection_week', \n",
    "                       'inpatient_bed_capacity', 'icu_bed_capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectColumnsTransfomer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    \n",
    "    Select columns by column names from pandas dataframes in scikit-learn pipelines.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    columns : list of column names \n",
    "    feature : Boolean\n",
    "        if True drop specify non-feature columns instead\n",
    "        else select specify relavant columns.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, columns=[], feature=False):\n",
    "        self.columns = columns\n",
    "        self.feature = feature\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\" \n",
    "        Selects columns of a DataFrame\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        trans : pandas DataFrame\n",
    "        pandas DataFrame contains selected columns of X      \n",
    "        \"\"\"\n",
    "        if self.feature:\n",
    "            X = X.drop(self.columns, axis=1)\n",
    "            return X\n",
    "        else: \n",
    "            X = X[self.columns].copy() \n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        \"\"\" Do nothing function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        y : default None\n",
    "                \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self  \n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Lag features \n",
    "Generate lag features for \n",
    "1. Hospital's Inpatient bed usage \n",
    "2. Hospital's Covid Inpatient cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dependant_columns = ['inpatient_bed_used', 'total_inpatients_confirmed_n_suspected_covid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateLagValues(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A DataFrame transformer that provides column selection\n",
    "    \n",
    "    Select columns by column names from pandas dataframes in scikit-learn pipelines.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    columns : list of str, names of the dataframe columns to select\n",
    "    lags : specify how much lag values to calculate\n",
    "        Example lags = 4 create\n",
    "        x-4 | x-3 | x-2 | x-1\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, columns=[], lags=4):\n",
    "        self.columns = columns\n",
    "        self.lags = lags\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\" Selects columns of a DataFrame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        trans : pandas DataFrame\n",
    "            contains selected columns of X      \n",
    "        \"\"\"\n",
    "        self.before_shape = X.shape\n",
    "        \n",
    "        for col in self.columns:\n",
    "            for lag in range(1, self.lags+1):\n",
    "                X = X.assign(**{f'{col}-{lag}': X.groupby('hospital_pk').shift(lag)[col]})\n",
    "        \n",
    "        return X.drop(self.columns, axis=1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        \"\"\" Do nothing function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        y : default None\n",
    "                \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self  \n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Imputer\n",
    "Impute missing value by the median value of the hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupByImputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    using median of group to impute (hospital_pk)\n",
    "    fill by 0 if all values in the group are Nan\n",
    "    '''\n",
    "    def __init__(self, group_column, targets=[]):\n",
    "        self.group_column = group_column\n",
    "        self.targets = targets\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        impute_map = X.groupby(self.group_column)[self.targets].median().reset_index(drop=False)\n",
    "        self.impute_map_ = impute_map\n",
    "        \n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        X = X.copy()\n",
    "        \n",
    "        for index, row in self.impute_map_.iterrows(): # loop through each hospital\n",
    "            group_mask = row[self.group_column] == X[self.group_column]\n",
    "            for col in self.targets:\n",
    "                X.loc[group_mask, col] = X.loc[group_mask, col].fillna(row[col])\n",
    "        X[self.targets] = X[self.targets].fillna(0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series training/validation/testing Split\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Splitting Strategy\n",
    "<pre>\n",
    "Training:   2020-07-31 to 2021-12-25 (5 months)  \n",
    "Validation: 2021-01-01 to 2021-01-22 (1 month)  \n",
    "Testing:    2021-01-29 to 2021-02-19 (1 month)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_split(X, y):\n",
    "    test_start_date =  X.collection_week.max()\n",
    "    train_last_date  = test_start_date - pd.to_timedelta(4,unit='W')    \n",
    "    test_idxs = X.loc[X.collection_week >= test_start_date].index\n",
    "    train_idx = X.loc[X.collection_week <= train_last_date].index\n",
    "    X_test, y_test = X.loc[test_idxs], y.loc[test_idxs]\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def time_series_cv(X, y):\n",
    "    '''\n",
    "    Create the custom Time Series cross validation object\n",
    "    Training:   2020-07-31 to 2021-12-25 (5 months) \n",
    "    Validation: 2021-01-01 to 2021-01-22 (1 month)  \n",
    "    '''\n",
    "    X = X.reset_index(drop=True)\n",
    "    valid_start_date =  X.collection_week.max()\n",
    "    train_last_date  = valid_start_date - pd.to_timedelta(4,unit='W') \n",
    "    valid_idxs = X.loc[X.collection_week >= valid_start_date].index\n",
    "    train_idx = X.loc[X.collection_week <= train_last_date].index\n",
    "    return [tuple([list(train_idx), list(valid_idxs)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = time_series_split(X_original, y_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and Optimizing Machine Learning Models & Hyperparameters\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Randomize Search for the best ML model & hyperparameter\n",
    "### Model Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Lasso (Multi-output)**\n",
    "<br>\n",
    "* Hyperparameters candidates  \n",
    " *  **alpha**: Different values of regularized coefficient(alpha) were choosen to explore the effect of regularization.\n",
    "\n",
    "**2. SGDRegressor (Multi-output)**\n",
    "* Hyperparameters candidates\n",
    " * **loss**: 2 types of loss functions (huber vs least sqaure) were choosen to explore the performance.\n",
    " * **penalty**: 2 types of regularization were choose to explore its effect (l2 & ElasticNet).\n",
    " * **alpha**: Different values of regularized coefficient(alpha) were choosen to explore the effect of regularization.\n",
    " \n",
    "**3. RandomForestRegressor (Multi-output)**\n",
    "* Hyperparameters candidates\n",
    " * **max_features**: 'sqrt', 'log2', 0.333 were choose to explore the effect of reducing trees correlation (reducing variance) by limiting max features\n",
    " * **n_estimators**: Different number of estimators were choosen to explore the performance.\n",
    " * **max_depth**: Different number of max depth were choosen to explore overfitting effect.\n",
    " * **min_samples_leaf**: Different number of min sampels leaf were choosen to explore overfitting effect.\n",
    "\n",
    "**4. GradientBoostingRegressor  (Multi-output)**\n",
    "* Hyperparameters candidates\n",
    " * **max_features**: 'sqrt', 'log2', 0.333 were choose to explore the effect of reducing trees correlation (reducing variance) by limiting max features.\n",
    " * **n_estimators**: Different number of estimators were choosen to explore the performance.\n",
    " * **max_depth**: Different number of max depth were choosen to explore overfitting effect.\n",
    " * **min_samples_leaf**: Different number of min sampels leaf were choosen to explore overfitting effect.\n",
    " * **loss**: 2 types of loss functions (huber vs least sqaure) were choosen to explore the performance.\n",
    " * **subsample**: Different subsample ratio were choose to explore if it could help to reduce the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "# Continuous value columns pipeline\n",
    "con_pipe = Pipeline([('scaler', StandardScaler())]) \n",
    "feature_lag_cols = []\n",
    "lags = 5\n",
    "for col in time_dependant_columns:\n",
    "    for lag in range(1, lags+1):\n",
    "        feature_lag_cols.append(f'{col}-{lag}')\n",
    "\n",
    "feature_con_cols = feature_lag_cols + ['avg_inpatient_bed_used']\n",
    "\n",
    "# Categorical value columns pipeline\n",
    "feature_cat_cols = ['hospital_subtype', 'is_metro_micro']\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\", add_indicator=True)),\n",
    "                     ('ohe', OneHotEncoder())])\n",
    "\n",
    "# Combine features pipeline\n",
    "to_feature = ColumnTransformer([('continuous',  con_pipe, feature_con_cols),\n",
    "                                ('categorical', cat_pipe, feature_cat_cols)])\n",
    "\n",
    "# Final pipeline\n",
    "pipeline = Pipeline([('calculateColumns', CreateCalculatedColumns()),\n",
    "                             ('selectColumns_1', SelectColumnsTransfomer(relevant_columns)),\n",
    "                             ('createTimelag', GenerateLagValues(time_dependant_columns, lags=lags)),\n",
    "                             ('custom_imputer', GroupByImputer('hospital_pk', targets=feature_con_cols)),\n",
    "                             ('selectColumns_2', SelectColumnsTransfomer(non_feature_columns, feature=True)),\n",
    "                             ('finalProcessing', to_feature),\n",
    "                             ('model', DummyEstimator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Best Models\n",
    "cv_time_series = time_series_cv(X_train, y_train)\n",
    "hyperparameters = [{'model': [Lasso(max_iter=3000)],\n",
    "                    'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1 ,10]},\n",
    "                   \n",
    "                   {'model': [SGDRegressor(max_iter=3000, early_stopping=True)],\n",
    "                    'model__loss': ['squared_loss', 'huber'],\n",
    "                    'model__penalty': ['l2', 'elasticnet'],\n",
    "                    'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1 ,10]},\n",
    "                   \n",
    "                   {'model': [RandomForestRegressor()],\n",
    "                    'model__max_features': ['sqrt', 'log2', 0.333],\n",
    "                    'model__n_estimators': np.arange(25, 201, 25),\n",
    "                    'model__max_depth': np.arange(10,31,5),\n",
    "                    'model__min_samples_leaf': [1, 3, 5, 10, 25, 100]},\n",
    "    \n",
    "                    {'model': [GradientBoostingRegressor()],\n",
    "                     'model__max_features': ['sqrt', 'log2', 0.333],\n",
    "                     'model__loss': ['ls', 'huber'],\n",
    "                     'model__n_estimators': np.arange(25,201,25),\n",
    "                     'model__max_depth': np.arange(10,31,5),\n",
    "                     'model__subsample': [0.8,0.9,1.0],\n",
    "                     'model__min_samples_leaf': np.arange(1,16,3)}]\n",
    "\n",
    "regr_rand_cv = RandomizedSearchCV(estimator=pipeline, \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=100, \n",
    "                              cv=cv_time_series, \n",
    "                              scoring='neg_mean_absolute_error',\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bank/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [         nan -21.22827525 -20.77078304          nan          nan\n",
      " -19.5893402           nan          nan          nan -22.10603561\n",
      " -19.34124895 -20.50383201          nan          nan          nan\n",
      "          nan -20.21889359          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan -19.70377385          nan          nan\n",
      " -20.36520721          nan          nan          nan -21.03207205\n",
      " -19.92231546          nan          nan -20.62294273 -19.95779161\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan -20.40146533\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan -21.64975775          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      " -19.77855632          nan          nan          nan          nan\n",
      "          nan -20.58924277          nan -21.80864581          nan\n",
      " -21.05714192          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n",
       "                         16, 17, 18, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, ...],\n",
       "                        [22, 45, 68, 91, 114, 137, 160, 183, 206, 229, 252, 275,\n",
       "                         298, 321, 344, 367, 390, 413, 436, 459, 482, 505, 528,\n",
       "                         551, 574, 597, 620, 643, 666, 689, ...])],\n",
       "                   estimator=Pipeline(steps=[('calculateColumns',\n",
       "                                              CreateCalculatedColumns()),\n",
       "                                             ('selectColumns_1',\n",
       "                                              SelectColumnsTransfomer(...\n",
       "                                         'model__n_estimators': array([ 25,  50,  75, 100, 125, 150, 175, 200])},\n",
       "                                        {'model': [GradientBoostingRegressor()],\n",
       "                                         'model__loss': ['ls', 'huber'],\n",
       "                                         'model__max_depth': array([10, 15, 20, 25, 30]),\n",
       "                                         'model__max_features': ['sqrt', 'log2',\n",
       "                                                                 0.333],\n",
       "                                         'model__min_samples_leaf': array([ 1,  4,  7, 10, 13]),\n",
       "                                         'model__n_estimators': array([ 25,  50,  75, 100, 125, 150, 175, 200]),\n",
       "                                         'model__subsample': [0.8, 0.9, 1.0]}],\n",
       "                   scoring='neg_mean_absolute_error', verbose=True)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_rand_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': 100,\n",
       " 'model__min_samples_leaf': 5,\n",
       " 'model__max_features': 0.333,\n",
       " 'model__max_depth': 25,\n",
       " 'model': RandomForestRegressor(max_depth=25, max_features=0.333, min_samples_leaf=5)}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_rand_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Final pipeline using best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('calculateColumns', CreateCalculatedColumns()),\n",
       "                ('selectColumns_1',\n",
       "                 SelectColumnsTransfomer(columns=['hospital_pk',\n",
       "                                                  'collection_week',\n",
       "                                                  'hospital_subtype',\n",
       "                                                  'is_metro_micro',\n",
       "                                                  'inpatient_bed_capacity',\n",
       "                                                  'icu_bed_capacity',\n",
       "                                                  'inpatient_bed_used',\n",
       "                                                  'avg_inpatient_bed_used',\n",
       "                                                  'total_inpatients_confirmed_n_suspected_covid'])),\n",
       "                ('createTimelag',\n",
       "                 GenerateLa...\n",
       "                                                   'total_inpatients_confirmed_n_suspected_covid-4',\n",
       "                                                   'total_inpatients_confirmed_n_suspected_covid-5',\n",
       "                                                   'avg_inpatient_bed_used']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['hospital_subtype',\n",
       "                                                   'is_metro_micro'])])),\n",
       "                ('model',\n",
       "                 RandomForestRegressor(max_depth=25, max_features=0.333,\n",
       "                                       min_samples_leaf=5))])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_final = Pipeline([('calculateColumns', CreateCalculatedColumns()),\n",
    "                             ('selectColumns_1', SelectColumnsTransfomer(relevant_columns)),\n",
    "                             ('createTimelag', GenerateLagValues(time_dependant_columns, lags=lags)),\n",
    "                             ('custom_imputer', GroupByImputer('hospital_pk', targets=feature_con_cols)),\n",
    "                             ('selectColumns_2', SelectColumnsTransfomer(non_feature_columns, feature=True)),\n",
    "                             ('finalProcessing', to_feature),\n",
    "                             ('model', RandomForestRegressor(max_depth=25, max_features=0.333, \n",
    "                                                             min_samples_leaf=5, n_estimators=100))])\n",
    "pipeline_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Stacking models with a Metalearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack 3 different models including\n",
    "1. Lasso\n",
    "2. Bayesian\n",
    "3. RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Search for the best of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 6 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 0.0001, 'model': Lasso(alpha=0.0001, max_iter=3000)}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_lasso = [{'model': [Lasso(max_iter=3000)],\n",
    "                    'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1 ,10]}]\n",
    "\n",
    "best_lasso_cv = RandomizedSearchCV(estimator=pipeline, \n",
    "                              param_distributions=hyperparameters_lasso, \n",
    "                              n_iter=6, \n",
    "                              cv=cv_time_series, \n",
    "                              scoring='neg_mean_absolute_error',\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True)\n",
    "best_lasso_cv.fit(X_train, y_train)\n",
    "best_lasso_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGDRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 12 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__estimator__penalty': 'l2',\n",
       " 'model__estimator__loss': 'huber',\n",
       " 'model__estimator__alpha': 0.0001,\n",
       " 'model': MultiOutputRegressor(estimator=SGDRegressor(early_stopping=True, loss='huber'))}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_sgd = [{'model': [MultiOutputRegressor(SGDRegressor(early_stopping=True))],\n",
    "                    'model__estimator__loss': ['huber'],\n",
    "                    'model__estimator__penalty': ['l2', 'elasticnet'],\n",
    "                    'model__estimator__alpha': [0.0001, 0.001, 0.01, 0.1, 1 ,10]}]\n",
    "\n",
    "best_sgd_cv = RandomizedSearchCV(estimator=pipeline, \n",
    "                              param_distributions=hyperparameters_sgd, \n",
    "                              n_iter=12, \n",
    "                              cv=cv_time_series, \n",
    "                              scoring='neg_mean_absolute_error',\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True)\n",
    "best_sgd_cv.fit(X_train, y_train)\n",
    "best_sgd_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 12 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': 75,\n",
       " 'model__min_samples_leaf': 3,\n",
       " 'model__max_features': 0.333,\n",
       " 'model__max_depth': 15,\n",
       " 'model': RandomForestRegressor(max_depth=15, max_features=0.333, min_samples_leaf=3,\n",
       "                       n_estimators=75)}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_rf = [{'model': [RandomForestRegressor()],\n",
    "                    'model__max_features': ['sqrt', 'log2', 0.333],\n",
    "                    'model__n_estimators': np.arange(25, 201, 25),\n",
    "                    'model__max_depth': np.arange(10,31,5),\n",
    "                    'model__min_samples_leaf': [1, 3, 5, 10, 25, 100]}]\n",
    "\n",
    "best_sgd_rf = RandomizedSearchCV(estimator=pipeline, \n",
    "                              param_distributions=hyperparameters_rf, \n",
    "                              n_iter=12, \n",
    "                              cv=cv_time_series, \n",
    "                              scoring='neg_mean_absolute_error',\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True)\n",
    "best_sgd_rf.fit(X_train, y_train)\n",
    "best_sgd_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bank/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n",
       "                         16, 17, 18, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, ...],\n",
       "                        [22, 45, 68, 91, 114, 137, 160, 183, 206, 229, 252, 275,\n",
       "                         298, 321, 344, 367, 390, 413, 436, 459, 482, 505, 528,\n",
       "                         551, 574, 597, 620, 643, 666, 689, ...])],\n",
       "                   estimator=Pipeline(steps=[('calculateColumns',\n",
       "                                              CreateCalculatedColumns()),\n",
       "                                             ('selectColumns_1',\n",
       "                                              SelectColumnsTransfomer(...\n",
       "                                                                                                                                              n_estimators=50)))],\n",
       "                                         'model__estimator__final_estimator__max_depth': array([10, 15, 20, 25, 30]),\n",
       "                                         'model__estimator__final_estimator__max_features': [0.33,\n",
       "                                                                                             'auto',\n",
       "                                                                                             'sqrt',\n",
       "                                                                                             'log2'],\n",
       "                                         'model__estimator__final_estimator__min_samples_leaf': [1,\n",
       "                                                                                                 3,\n",
       "                                                                                                 5,\n",
       "                                                                                                 10,\n",
       "                                                                                                 25],\n",
       "                                         'model__estimator__final_estimator__n_estimators': [50,\n",
       "                                                                                             100,\n",
       "                                                                                             150,\n",
       "                                                                                             200]}],\n",
       "                   scoring='neg_mean_absolute_error', verbose=True)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [('ridge', Lasso(max_iter=3000,alpha=0.0001)),\n",
    "              ('SGDRegressor', SGDRegressor(max_iter=3000, \n",
    "                                            penalty='l2', \n",
    "                                            loss='huber', \n",
    "                                            alpha=0.0001)),\n",
    "              ('rf',   RandomForestRegressor(max_depth=25, max_features=0.333, \n",
    "                                             min_samples_leaf=5, n_estimators=100))]\n",
    "\n",
    "final_estimator = GradientBoostingRegressor()\n",
    "\n",
    "stacking_regressor = MultiOutputRegressor(\n",
    "    StackingRegressor(estimators=estimators,\n",
    "                      final_estimator=final_estimator))\n",
    "\n",
    "hyperparameters = [{'model': [stacking_regressor],\n",
    "                    'model__estimator__final_estimator__n_estimators': [50,100,150,200],\n",
    "                    'model__estimator__final_estimator__max_features': [0.33, 'auto', 'sqrt', 'log2'],\n",
    "                    'model__estimator__final_estimator__max_depth': np.arange(10,31,5),\n",
    "                    'model__estimator__final_estimator__min_samples_leaf': [1, 3, 5, 10, 25]}]\n",
    "\n",
    "best_final_stacking = RandomizedSearchCV(estimator=pipeline, \n",
    "                                      param_distributions=hyperparameters, \n",
    "                                      n_iter=20, \n",
    "                                      cv=cv_time_series, \n",
    "                                      scoring='neg_mean_absolute_error',\n",
    "                                      n_jobs=-1,\n",
    "                                      verbose=True)\n",
    "best_final_stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__estimator__final_estimator__n_estimators': 200,\n",
       " 'model__estimator__final_estimator__min_samples_leaf': 5,\n",
       " 'model__estimator__final_estimator__max_depth': 20,\n",
       " 'model__estimator__final_estimator__loss': 'ls',\n",
       " 'model': MultiOutputRegressor(estimator=StackingRegressor(estimators=[('ridge',\n",
       "                                                               Lasso(alpha=0.0001,\n",
       "                                                                     max_iter=3000)),\n",
       "                                                              ('SGDRegressor',\n",
       "                                                               SGDRegressor(loss='huber',\n",
       "                                                                            max_iter=3000)),\n",
       "                                                              ('rf',\n",
       "                                                               RandomForestRegressor(max_depth=25,\n",
       "                                                                                     max_features=0.333,\n",
       "                                                                                     min_samples_leaf=5))],\n",
       "                                                  final_estimator=GradientBoostingRegressor(max_depth=20,\n",
       "                                                                                            max_features=0.333,\n",
       "                                                                                            min_samples_leaf=5,\n",
       "                                                                                            n_estimators=200)))}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_final_stacking.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Final pipeline using best parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('calculateColumns', CreateCalculatedColumns()),\n",
       "                ('selectColumns_1',\n",
       "                 SelectColumnsTransfomer(columns=['hospital_pk',\n",
       "                                                  'collection_week',\n",
       "                                                  'hospital_subtype',\n",
       "                                                  'is_metro_micro',\n",
       "                                                  'inpatient_bed_capacity',\n",
       "                                                  'icu_bed_capacity',\n",
       "                                                  'inpatient_bed_used',\n",
       "                                                  'avg_inpatient_bed_used',\n",
       "                                                  'total_inpatients_confirmed_n_suspected_covid'])),\n",
       "                ('createTimelag',\n",
       "                 GenerateLa...\n",
       "                ('model',\n",
       "                 MultiOutputRegressor(estimator=StackingRegressor(estimators=[('ridge',\n",
       "                                                                               Lasso(alpha=0.0001,\n",
       "                                                                                     max_iter=3000)),\n",
       "                                                                              ('SGDRegressor',\n",
       "                                                                               SGDRegressor(loss='huber',\n",
       "                                                                                            max_iter=3000)),\n",
       "                                                                              ('rf',\n",
       "                                                                               RandomForestRegressor(max_depth=25,\n",
       "                                                                                                     max_features=0.333,\n",
       "                                                                                                     min_samples_leaf=5))],\n",
       "                                                                  final_estimator=GradientBoostingRegressor(max_features=0.333,\n",
       "                                                                                                            min_samples_leaf=25,\n",
       "                                                                                                            n_estimators=150))))])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_estimator = GradientBoostingRegressor(max_features=0.333,\n",
    "                                            min_samples_leaf=25,\n",
    "                                            n_estimators=150)\n",
    "\n",
    "\n",
    "stacking_regressor = MultiOutputRegressor(\n",
    "    StackingRegressor(estimators=estimators,\n",
    "                      final_estimator=final_estimator)\n",
    "    )\n",
    "\n",
    "pipeline_stacking = Pipeline([('calculateColumns', CreateCalculatedColumns()),\n",
    "                             ('selectColumns_1', SelectColumnsTransfomer(relevant_columns)),\n",
    "                             ('createTimelag', GenerateLagValues(time_dependant_columns, lags=lags)),\n",
    "                             ('custom_imputer', GroupByImputer('hospital_pk', targets=feature_con_cols)),\n",
    "                             ('selectColumns_2', SelectColumnsTransfomer(non_feature_columns, feature=True)),\n",
    "                             ('finalProcessing', to_feature),\n",
    "                             ('model', stacking_regressor)])\n",
    "pipeline_stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Multioutput+Stacking Bug in Sci-kit Learn Ref: https://github.com/scikit-learn/scikit-learn/issues/16549\n",
    "pipeline_stacking.steps[-1][1].estimator.estimators_ = pipeline_stacking.steps[-1][1].estimator.estimators\n",
    "pipeline_stacking.steps[-1][1].estimator.final_estimator_ = pipeline_stacking.steps[-1][1].estimator.final_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluation\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metric**\n",
    "- Uniform average of **mean absolote error (MAE)** was chosen as evaluation metric as mean absolote error unit is easily interpret by layperson. The error unit is the number of bed usage that we wrongly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Best ML model(RF): 12.262304999642728\n"
     ]
    }
   ],
   "source": [
    "# Final best Model\n",
    "y_pred = pipeline_final.predict(X_test)\n",
    "mae_y_pred = mean_absolute_error(y_test.values, y_pred, multioutput='uniform_average')\n",
    "print('MAE Best ML model(RF):', mae_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Stacking Ensemble model: 12.4845304135933\n"
     ]
    }
   ],
   "source": [
    "# Stacking Model\n",
    "y_pred_stacking = pipeline_stacking.predict(X_test)\n",
    "mae_y_pred_stacking = mean_absolute_error(y_test.values, y_pred_stacking, multioutput='uniform_average')\n",
    "print('MAE Stacking Ensemble model:', mae_y_pred_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Naive baseline (Forecast by latest week): 15.431180643348785\n"
     ]
    }
   ],
   "source": [
    "# Naive Model\n",
    "baseline = np.hstack([X_original[X_original.collection_week == '2021-01-22'][['inpatient_bed_used']].values]*4)\n",
    "mae_baseline = mean_absolute_error(y_test.values, baseline, multioutput='uniform_average')\n",
    "print('MAE Naive baseline (Forecast by latest week):', mae_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Best Model (Random Forest): 12.26**  \n",
    "2. Stacking Ensemble: 12.48  \n",
    "3. Naive Model: 15.43  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of Machine learning model were choosen for comparison during randomized hyperparameters search.\n",
    "For instance, \n",
    "* Regularlized Linear Regression (l2, l1, ElasticNet)\n",
    "* Bagging Ensemble Model (RandomForest)\n",
    "* Boosting Ensemble Model (GradientBoosting)  \n",
    "\n",
    "<br>\n",
    "After comparing all models using RandomizedSearchCV, RandomForest was selected as the final model for forecasting.\n",
    "<br>\n",
    "It achieved the lowest MAE (MAE=12.26), with comparison to the Naive model, it has 20% lower MAE.\n",
    "<br>\n",
    "In the latter section, the Stacking Ensemble Model from linear models & RandomForest was also tested.\n",
    "<br>\n",
    "The performance of the stacking ensemble model was very close to RandomForest model with MAE=12.48. \n",
    "<br>\n",
    "The result from the experiment indicated that Ensemble Models perform well for forecasting hospital capacity\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
